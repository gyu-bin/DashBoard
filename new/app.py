import os
import tempfile
import streamlit as st
import pandas as pd
import numpy as np

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="PDF ê¸°ë°˜ Q&A ì‹œìŠ¤í…œ",
    page_icon="ğŸ“„",
    layout="wide"
)

st.title("ğŸ“„ PDF ê¸°ë°˜ Q&A ì‹œìŠ¤í…œ")
st.markdown("ì—…ë¡œë“œí•œ PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.")

# API í‚¤ ì…ë ¥ (Streamlitì—ì„œ getpass ëŒ€ì‹  text_input ì‚¬ìš©)
api_key = st.text_input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", type="password")
if api_key:
    os.environ["OPENAI_API_KEY"] = api_key

###########################################
# PDF ì—…ë¡œë“œ ë° ì²˜ë¦¬ í•¨ìˆ˜ (Streamlit ë²„ì „)
###########################################
def upload_and_process_pdf():
    """
    ì‚¬ìš©ìê°€ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì´ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (Streamlit ë²„ì „)
    """
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”...", type=["pdf"])
    if uploaded_file is None:
        st.warning("íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None, None

    filename = uploaded_file.name

    # ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ì— ì €ì¥
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pdf')
    temp_file.write(uploaded_file.read())
    temp_path = temp_file.name
    temp_file.close()

    st.info(f"'{filename}' íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # PDF íŒŒì¼ ë¡œë“œ
    loader = PyPDFLoader(temp_path)
    documents = loader.load()
    st.info(f"PDFì—ì„œ {len(documents)} í˜ì´ì§€ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_documents(documents)
    st.info(f"ë¬¸ì„œë¥¼ {len(chunks)} ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    os.unlink(temp_path)

    return chunks, filename

document_chunks, pdf_filename = upload_and_process_pdf()
if document_chunks is None:
    st.stop()

###########################################
# ë²¡í„° ìŠ¤í† ì–´ ìƒì„± í•¨ìˆ˜
###########################################
def create_vector_store(chunks):
    if chunks is None or len(chunks) == 0:
        st.error("ì²˜ë¦¬í•  ë¬¸ì„œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    st.info("ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
    option = st.radio("ì„ë² ë”© ëª¨ë¸ ì„ íƒ", ("OpenAI ì„ë² ë”©", "HuggingFace ì„ë² ë”©"))
    if option == "OpenAI ì„ë² ë”©":
        embeddings = OpenAIEmbeddings()
        st.info("OpenAI ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
        st.info("HuggingFace ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    try:
        vector_store = FAISS.from_documents(chunks, embeddings)
        st.success("ë²¡í„° ìŠ¤í† ì–´ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return vector_store
    except Exception as e:
        st.error(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

vector_store = create_vector_store(document_chunks)
if vector_store is None:
    st.stop()

###########################################
# RAG ì²´ì¸ ì„¤ì • í•¨ìˆ˜
###########################################
def setup_rag_chain(vector_store):
    if vector_store is None:
        st.error("ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ì–´ RAG ì²´ì¸ì„ ì„¤ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    st.info("RAG ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤...")
    # ëª¨ë¸ ì„ íƒ
    model_choice = st.radio("ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ", ("gpt-3.5-turbo", "gpt-4"))
    if model_choice == "gpt-4":
        llm = ChatOpenAI(temperature=0, model="gpt-4")
        st.info("GPT-4 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo")
        st.info("GPT-3.5-turbo ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # ëŒ€í™” ë©”ëª¨ë¦¬ ì„¤ì •
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    # ê²€ìƒ‰ê¸° ì„¤ì • (ìƒìœ„ 3ê°œ ì²­í¬ ê²€ìƒ‰)
    retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 3})

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
    qa_template = """
    ë‹¹ì‹ ì€ PDF ë¬¸ì„œì˜ ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

    ì£¼ì–´ì§„ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´, "ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

    í•­ìƒ ë¬¸ì„œì˜ ë‚´ìš©ì— ì¶©ì‹¤í•˜ê²Œ ë‹µë³€í•˜ê³ , ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.

    ì§ˆë¬¸: {question}

    ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:
    {context}

    ë‹µë³€:
    """
    QA_PROMPT = PromptTemplate(template=qa_template, input_variables=["question", "context"])

    # RAG ì²´ì¸ êµ¬ì„±
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        combine_docs_chain_kwargs={"prompt": QA_PROMPT}
    )
    st.success("RAG ì²´ì¸ êµ¬ì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    return qa_chain

qa_chain = setup_rag_chain(vector_store)
if qa_chain is None:
    st.stop()

###########################################
# ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ (Streamlit ë°©ì‹)
###########################################
st.header(f"{pdf_filename} ë¬¸ì„œì™€ ëŒ€í™”í•˜ê¸°")
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
if st.button("ì „ì†¡"):
    if question.strip() == "":
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        st.session_state.chat_history.append({"role": "user", "content": question})
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            response = qa_chain({"question": question})
            answer = response.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
        st.session_state.chat_history.append({"role": "assistant", "content": answer})
        st.success("ë‹µë³€ ìƒì„± ì™„ë£Œ!")

# ëŒ€í™” ì´ë ¥ í‘œì‹œ
if st.session_state.chat_history:
    st.subheader("ëŒ€í™” ì´ë ¥")
    for msg in st.session_state.chat_history:
        if msg["role"] == "user":
            st.markdown(f"**ì§ˆë¬¸:** {msg['content']}")
        else:
            st.markdown(f"**ë‹µë³€:** {msg['content']}")

###########################################
# ëŒ€í™” ì´ë ¥ ì‹œê°í™” í•¨ìˆ˜ (ì˜µì…˜)
###########################################
def visualize_conversation_history(qa_chain):
    if qa_chain is None or not hasattr(qa_chain, 'memory') or qa_chain.memory is None:
        st.error("ëŒ€í™” ì´ë ¥ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    chat_history = qa_chain.memory.chat_memory.messages
    if not chat_history:
        st.info("ì•„ì§ ëŒ€í™” ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    st.subheader("ë©”ëª¨ë¦¬ ê¸°ë°˜ ëŒ€í™” ì´ë ¥")
    for i, message in enumerate(chat_history):
        if hasattr(message, 'type') and message.type == 'human':
            st.markdown(f"**ì§ˆë¬¸ {i//2 + 1}:** {message.content}")
        elif hasattr(message, 'type') and message.type == 'ai':
            st.markdown(f"**ë‹µë³€ {i//2 + 1}:** {message.content}")
            st.markdown("---")

# ëŒ€í™” ì´ë ¥ ì‹œê°í™” ë²„íŠ¼
if st.button("ë©”ëª¨ë¦¬ ëŒ€í™” ì´ë ¥ ë³´ê¸°"):
    visualize_conversation_history(qa_chain)

###########################################
# ë‹¤ì¤‘ PDF ì²˜ë¦¬ (ì˜µì…˜)
###########################################
st.header("ì—¬ëŸ¬ PDF íŒŒì¼ ì—…ë¡œë“œ ë° ì²˜ë¦¬ (ì˜µì…˜)")
multi_pdf_uploaded = st.file_uploader("ì—¬ëŸ¬ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (ì—¬ëŸ¬ íŒŒì¼ ì„ íƒ ê°€ëŠ¥)", type=["pdf"], accept_multiple_files=True)
if multi_pdf_uploaded:
    all_chunks = []
    filenames = []
    for file in multi_pdf_uploaded:
        if not file.name.lower().endswith('.pdf'):
            st.warning(f"'{file.name}'ì€(ëŠ”) PDF íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pdf')
        temp_file.write(file.read())
        temp_path = temp_file.name
        temp_file.close()

        st.info(f"'{file.name}' íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
        try:
            loader = PyPDFLoader(temp_path)
            documents = loader.load()
            st.info(f"- {len(documents)} í˜ì´ì§€ ë¡œë“œë¨.")

            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)
            chunks = text_splitter.split_documents(documents)
            st.info(f"- {len(chunks)} ì²­í¬ë¡œ ë¶„í• ë¨.")

            # ê° ì²­í¬ì— íŒŒì¼ëª… ì¶”ê°€
            for chunk in chunks:
                if 'source' not in chunk.metadata:
                    chunk.metadata['source'] = file.name

            all_chunks.extend(chunks)
            filenames.append(file.name)
        except Exception as e:
            st.error(f"'{file.name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            os.unlink(temp_path)

    if all_chunks:
        st.success(f"ì´ {len(all_chunks)} ì²­í¬ê°€ {len(filenames)}ê°œì˜ PDFì—ì„œ ì¶”ì¶œë¨.")
        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (ì˜µì…˜)
        try:
            embeddings = OpenAIEmbeddings()
            multi_vector_store = FAISS.from_documents(all_chunks, embeddings)
            st.success("ì—¬ëŸ¬ PDF ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!")
            multi_qa_chain = setup_rag_chain(multi_vector_store)
            st.header("ì—¬ëŸ¬ PDF ë¬¸ì„œì™€ì˜ ëŒ€í™”")
            multi_question = st.text_input("ì—¬ëŸ¬ PDFì— ëŒ€í•œ ì§ˆë¬¸ ì…ë ¥:", key="multi_q")
            if st.button("ì „ì†¡ (ì—¬ëŸ¬ PDF)"):
                if multi_question.strip() == "":
                    st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                else:
                    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                        multi_response = multi_qa_chain({"question": multi_question})
                        multi_answer = multi_response.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
                    st.success("ë‹µë³€ ìƒì„± ì™„ë£Œ!")
                    st.markdown(f"**ë‹µë³€:** {multi_answer}")
        except Exception as e:
            st.error(f"ë‹¤ì¤‘ PDF ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
    else:
        st.warning("ì²˜ë¦¬ëœ PDF ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

st.markdown("---")
st.caption("Â© 2023 í•™ìƒ ì„±ì  ê´€ë¦¬ ëŒ€ì‹œë³´ë“œ | Streamlitìœ¼ë¡œ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤")
